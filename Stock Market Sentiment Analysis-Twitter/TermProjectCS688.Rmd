---
title: "CS688 Term Project"
output: html_notebook
---

# 1. Connecting to Twitter API

 ----------- Loading Required Libraries ------

```{r}
install.packages("twitteR")
install.packages("tm")
install.packages("wordcloud")
library(twitteR)
library(tm)
library(wordcloud)
```

---------  Authenticating Twitter App --------- 

```{r}
t.api.key<-"ieaUlcnfMv8vSXYNkX5wFBZTK"
t.api.secret<-"bEbt6ekRgOvWiv8LwsV8L4uzIzhVmU5eoMCl8iQ2ZLUmb29dSx"
setup_twitter_oauth(t.api.key, t.api.secret, access_token=NULL, access_secret=NULL)
save(list=(c("t.api.key","t.api.secret")), file="twitter_credentials.RData")

```

# 2. Extracting 100 tweets for the stock cashtags 
```{r}
# GAINER STOCKS
tweets.stv <- searchTwitter('$STV',n=100)
tweets.aktx <- searchTwitter('$AKTX',n=100)
tweets.carv <- searchTwitter('$CARV',n=100)
gainer.tweets<-c(tweets.stv,tweets.aktx,tweets.carv)

# LOSER STOCKS
tweets.xbit <- searchTwitter('$XBIT',n=100)
tweets.ontx <- searchTwitter('$ONTX',n=100)
tweets.gnmx <- searchTwitter('$GNMX',n=100)
loser.tweets<-c(tweets.xbit,tweets.ontx,tweets.gnmx)

# Converting both lists to ASCII

gainer.tweets.text <-
  
  lapply(gainer.tweets,
         
         function(t) {
           
           iconv(t$getText(),
                 
                 "latin1", "ASCII", sub="")
           
         })

loser.tweets.text <-
  
  lapply(loser.tweets,
         
         function(t) {
           
           iconv(t$getText(),
                 
                 "latin1", "ASCII", sub="")
           
         })


# Verifying accuracy of data

head(gainer.tweets.text,n=2)
length(gainer.tweets.text)
typeof(gainer.tweets.text)

head(loser.tweets.text,n=2)
length(loser.tweets.text)
typeof(loser.tweets.text)



```
# 3. Creating Data Corpus for  tweets sets
Inputs:
- gainer.tweets.text
- loser.tweets.text

```{r}
# function that takes a tweet and returns a corpus
createCorpus<- function(tweetset){
data.source <-VectorSource(tweetset)
data.corpus<-Corpus(data.source)
return(data.corpus)
}

# Creating corpora using the createCorpus function
data.corpus1<-createCorpus(gainer.tweets.text)
data.corpus2<-createCorpus(loser.tweets.text)
typeof(data.corpus1)
# Inspecting the corpora
inspect(data.corpus1[1:2])
inspect(data.corpus2[1:2])

```

# 4. TRANSFORMATIONS

Implement the pre-processing as a function that takes a corpus and returns a pre-processed corpus.
inputs:
data.corpus1
data.corpus2

```{r}

# Defining the transformCorpus() function
transformCorpus<- function(data.corpus){
content_transformer(function(x, from, to) gsub(from, to, x))

# convert to lowercase
data.corpus <- 
  tm_map(data.corpus, 
         content_transformer(tolower))

# Defining URL elements for removing URL from text
removeURL <- function(x) {
  gsub("(http[^ ]*)", "", x)
}

# remove URL
data.corpus <- 
  tm_map(data.corpus, 
         content_transformer(removeURL))

# Defining stop words in this context 
english.stopwords <- stopwords("en")
head(english.stopwords)

# remove stopwords  
data.corpus <- 
  tm_map(data.corpus,
         content_transformer(removeWords),
         english.stopwords)


# Remove punctuation 

data.corpus <- 
  tm_map(data.corpus, 
         content_transformer(removePunctuation))


# remove words that begin with a number 
removeNumberWords <- function(x) {
  gsub("([[:digit:]]+)([[:alnum:]])*", "", x)
}

data.corpus <- 
  tm_map(data.corpus, 
         content_transformer(removeNumberWords))

# Perform stemming

data.corpus <- 
  tm_map(data.corpus,
        content_transformer(stemDocument))


# Strip white space 
data.corpus <- 
  tm_map(data.corpus,
         content_transformer(stripWhitespace))

return(data.corpus)
}

# Performing transformation
data.corpus1<-transformCorpus(data.corpus1)
data.corpus2<-transformCorpus(data.corpus2)

# Inspecting the two Corpora after applying transformations
inspect(data.corpus1[1:2])
inspect(data.corpus2[1:2])



```




# 5. Creating the Term Document Matrix -- 
--Inputs:
--  data.corpus1
--  data.corpus2

```{r}

# CONSTRUCTING tdm1
# Build the term document matrix for data.corpus1 and inspect part of matrix

tdm1 <- TermDocumentMatrix(data.corpus1)
inspect(tdm1[1:10, 50:60])

# convert TDM to a matrix and view portion of the matrix

m1 <- as.matrix(tdm1)
m1[1:10, 51:60]


# CONSTRUCTING tdm2
# Build the term document matrix for data.corpus2 and inspect part of matrix
tdm2 <- TermDocumentMatrix(data.corpus2)
inspect(tdm2[1:10, 50:60])
# convert TDM to a matrix and view portion of the matrix
m2 <- as.matrix(tdm2)
m2[1:10, 51:60]



# Saving tdm1 and tdm2 as R objects

save(list=("tdm1"),
     file = "GainerTDM.RData")

load(file="GainerTDM.RData")


save(list=("tdm2"),
     file = "LoserTDM.RData")

load(file="LoserTDM.RData")
```
# 6. Determing frequent words and Generate word cloud

-- Calculate the frequency of words, and examine part of the frequencies--
-- Sort the words by descending order of frequency--
-- NOTE: the first 3 rows containing
 stock names are removed to get a better wordcloud representation
```{r}
# calculate the frequency of words in m1
wordFreq1 <- rowSums(m1)
wordFreq1 <- sort(wordFreq1, decreasing=TRUE)

# Examine the top ten words
cbind(wordFreq1[1:10])

# All frequent terms having atleast 10 occurrences
findFreqTerms(tdm1, lowfreq=10)


# calculate the frequency of words in m2
wordFreq2 <- rowSums(m2)
wordFreq2 <- sort(wordFreq2, decreasing=TRUE)
# Examine the top ten words
cbind(wordFreq2[1:10])

# frequent terms
findFreqTerms(tdm2, lowfreq=10)


#Removing stock names from  WordFreq1 and WordFreq2
wordFreq1 <- sort(wordFreq1[-c(1,2,3)], decreasing=TRUE)
cbind(wordFreq1[1:10])

wordFreq2 <- sort(wordFreq2[-c(1,2,3)], decreasing=TRUE)
cbind(wordFreq2[1:10])

```
```{r}
# Word Cloud Generation
library(wordcloud)
genWordCloud <- function(wordFreqList){
palette <- brewer.pal(8,"Dark2")
set.seed(123)
wordcloud(words=names(wordFreqList), 
          freq=wordFreqList, 
          min.freq=10, 
          random.order=F,
          colors=palette)
}

genWordCloud(wordFreq1)
genWordCloud(wordFreq2)


```

# 7. --- SENTIMENT ANALYSIS


```{r}
getwd()
setwd("/Users/archanabalachandran/Documents/")

# Lexicons
pos.words = scan('positive-words.txt',
                 what='character',
                 comment.char=';')

neg.words = scan('negative-words.txt',  
                 what='character', 
                 comment.char=';')

head(pos.words)

head(neg.words)



# Sentiment determining function

sentiment <- function(text, pos.words, neg.words) {
  text <- gsub('[[:punct:]]', '', text)
  text <- gsub('[[:cntrl:]]', '', text)
  text <- gsub('\\d+', '', text)
  text <- tolower(text)
  # split the text into a vector of words
  words <- strsplit(text, '\\s+')
  words <- unlist(words)
  # find which words are positive
  pos.matches <- match(words, pos.words)
  pos.matches <- !is.na(pos.matches)
  # find which words are negative
  neg.matches <- match(words, neg.words)
  neg.matches <- !is.na(neg.matches)
  # calculate the sentiment score
  p <- sum(pos.matches)
  n <- sum(neg.matches)
  if (p == 0 & n == 0)
    return (NA)
  else
    return (p - n)
}


# Verifying sentiment score for specific Gainer tweets
gainer.tweets.text[[2]]
sentiment(gainer.tweets.text[[2]], pos.words, neg.words)

gainer.tweets.text[[5]]
sentiment(gainer.tweets.text[[5]], pos.words, neg.words)

gainer.tweets.text[[50]]
sentiment(gainer.tweets.text[[50]], pos.words, neg.words)

gainer.tweets.text[[4]]
sentiment(gainer.tweets.text[[4]], pos.words, neg.words)

# Verifying sentiment score for specific Loser tweets
loser.tweets.text[[2]]
sentiment(loser.tweets.text[[2]], pos.words, neg.words)

loser.tweets.text[[5]]
sentiment(loser.tweets.text[[5]], pos.words, neg.words)

loser.tweets.text[[50]]
sentiment(loser.tweets.text[[50]], pos.words, neg.words)

loser.tweets.text[[4]]
sentiment(loser.tweets.text[[4]], pos.words, neg.words)



```


```{r}
sink(tempfile())

# SENTIMENT SCORE FOR GAINER TWEETS
gainer.scores <- sapply(gainer.tweets.text, 
                    sentiment, 
                    pos.words, neg.words)
table(gainer.scores)


barplot(table(gainer.scores), 
        xlab="Score", ylab="Count",
        ylim=c(0,80), col="cadetblue3")

# Computing overall sentiment

total.neg.score <-table(gainer.scores)[[1]]+table(gainer.scores)[[2]]+table(gainer.scores)[[3]]
total.neg.score

total.neutral.score <-table(gainer.scores)[[4]]
total.neutral.score

total.pos.score<-table(gainer.scores)[[5]]+table(gainer.scores)[[6]]
total.pos.score

if(total.pos.score>total.neg.score)
   {
print("Positive Sentiment")
} else { print("Not positive sentiment")}


# Data frame of scores and tweets

stv.vector <- sapply(gainer.tweets.text,
                      function (t) {(t)})
x1 <- data.frame(Score=scores, Text=stv.vector)
View(x1)

```


```{r}
# SENTIMENT SCORE FOR LOSER TWEETS
loser.scores <- sapply(loser.tweets.text, 
                    sentiment, 
                    pos.words, neg.words)
table(loser.scores)
barplot(table(loser.scores), 
        xlab="Score", ylab="Count",
        ylim=c(0,80), col="cadetblue3")



total.neg.score <-table(loser.scores)[[1]]+table(loser.scores)[[2]]+table(loser.scores)[[3]]
total.neg.score

total.neutral.score <-table(loser.scores)[[4]]
total.neutral.score

total.pos.score<-table(loser.scores)[[5]]+table(loser.scores)[[6]]
total.pos.score

if(total.pos.score<total.neg.score)
   {
print("Negative Sentiment")
} else { print("Not Negative sentiment")}

# Data frame of scores and tweets

stv.vector <- sapply(loser.tweets.text,
                      function (t) {(t)})
x2 <- data.frame(Score=loser.scores, Text=stv.vector)
View(x2)
```


```{r}
# Function for LOSER ANALYSIS

# Analyzing individual stock info for loser stock-because the positive and negative sentiments are almost the same

analyze.loser.tweet <-function(losertweet){
# Computing overall sentiment
sentiment <- function(text, pos.words, neg.words) {
  text <- gsub('[[:punct:]]', '', text)
  text <- gsub('[[:cntrl:]]', '', text)
  text <- gsub('\\d+', '', text)
  text <- tolower(text)
  # split the text into a vector of words
  words <- strsplit(text, '\\s+')
  words <- unlist(words)
  # find which words are positive
  pos.matches <- match(words, pos.words)
  pos.matches <- !is.na(pos.matches)
  # find which words are negative
  neg.matches <- match(words, neg.words)
  neg.matches <- !is.na(neg.matches)
  # calculate the sentiment score
  p <- sum(pos.matches)
  n <- sum(neg.matches)
  if (p == 0 & n == 0)
    return (NA)
  else
    return (p - n)
}


loser.tweets.text <-
  
  lapply(losertweet,
         
         function(t) {
           
           iconv(t$getText(),
                 
                 "latin1", "ASCII", sub="")
           
         })
# SENTIMENT SCORE FOR LOSER TWEETS - XBIT
loser.scores <- sapply(loser.tweets.text, 
                    sentiment, 
                    pos.words, neg.words)
print(table(loser.scores))
barplot(table(loser.scores), 
        xlab="Score", ylab="Count",
        ylim=c(0,80), col="cadetblue3")

}
analyze.loser.tweet(tweets.xbit)
analyze.loser.tweet(tweets.ontx)
analyze.loser.tweet(tweets.gnmx)
analyze.loser.tweet(loser.tweets)


```





```{r}
install.packages("googleVis")
library(googleVis)

trend.chart<- gvisBarChart(data)
plto(trend.chart)
```
# BONUS SECTION: googleVis
```{r}
# installing and loading required packages
#install.packages("quantmod")
#install.packages("googleVis")
library(quantmod) # For obtaining stock prices
library(googleVis)

# Obtaining stock prices for STV,AKTX,CARV, XBIT, GNMX, ONTX
stock.quotes <- getQuote("STV;AKTX;CARV;XBIT;GNMX;ONTX")
typeof(stock.quotes)
#Extracting the % Change values and storing as characted vector
stock.quotes.char <- stock.quotes$`% Change`
# cleaning data and converting to numeric format 
# E.g. "+34.65%" convert to 34.65
# Remove double quotes and % sign and convert to numeric vector
db <- gsub(",","",stock.quotes.char)
db <- gsub("%","",db)
db<-as.numeric(db)

# Converting numeric vector to dataframe for googleVis
data <- data.frame(percent.change=db)


df=data.frame(stock=c("STV", "AKTX", "CARV", "XBIT","GNMX","ONTX"), 
              val1=data)

# Generating Bar Chart
trend.chart<- gvisBarChart(df)
plot(trend.chart)


```

------ END OF PROJECT --------
